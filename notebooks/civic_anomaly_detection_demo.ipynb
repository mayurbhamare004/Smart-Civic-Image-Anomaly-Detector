{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Civic Image Anomaly Detection - Demo Notebook\n",
    "\n",
    "This notebook demonstrates how to use the Civic Image Anomaly Detector for detecting urban infrastructure issues.\n",
    "\n",
    "## ðŸŽ¯ What We'll Cover\n",
    "1. Setup and imports\n",
    "2. Load the trained model\n",
    "3. Process sample images\n",
    "4. Visualize results\n",
    "5. Analyze detection statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install ultralytics opencv-python matplotlib seaborn pandas numpy pillow\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Core imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# Project imports\n",
    "from scripts.inference import CivicAnomalyDetector\n",
    "from app.utils import calculate_detection_stats, create_detection_report\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize the Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the civic anomaly detector\n",
    "model_path = \"../models/weights/civic_detector_final.pt\"\n",
    "\n",
    "# Check if custom model exists, otherwise use pretrained YOLOv8\n",
    "if Path(model_path).exists():\n",
    "    detector = CivicAnomalyDetector(model_path)\n",
    "    print(f\"âœ… Loaded custom model: {model_path}\")\n",
    "else:\n",
    "    print(\"âš ï¸  Custom model not found. Using pretrained YOLOv8n...\")\n",
    "    detector = CivicAnomalyDetector()\n",
    "    print(\"âœ… Loaded pretrained model\")\n",
    "\n",
    "# Display class information\n",
    "print(\"\\nðŸŽ¯ Detection Classes:\")\n",
    "for class_id, class_name in detector.class_names.items():\n",
    "    print(f\"  {class_id}: {class_name.replace('_', ' ').title()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_detection_results(image_path, result_image, detections, figsize=(15, 8)):\n",
    "    \"\"\"Display original and result images side by side\"\"\"\n",
    "    \n",
    "    # Load original image\n",
    "    original = cv2.imread(str(image_path))\n",
    "    original_rgb = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Convert result to RGB\n",
    "    result_rgb = cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Create subplot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # Original image\n",
    "    ax1.imshow(original_rgb)\n",
    "    ax1.set_title(f\"Original Image\\n{Path(image_path).name}\", fontsize=14, fontweight='bold')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Result image\n",
    "    ax2.imshow(result_rgb)\n",
    "    ax2.set_title(f\"Detection Results\\n{len(detections)} anomalies found\", fontsize=14, fontweight='bold')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display detection details\n",
    "    if detections:\n",
    "        print(\"\\nðŸ” Detection Details:\")\n",
    "        for i, det in enumerate(detections, 1):\n",
    "            print(f\"  {i}. {det['class_name'].replace('_', ' ').title()}: {det['confidence']:.2f}\")\n",
    "    else:\n",
    "        print(\"\\nâœ… No civic anomalies detected!\")\n",
    "\n",
    "def plot_detection_statistics(all_detections):\n",
    "    \"\"\"Plot detection statistics\"\"\"\n",
    "    if not all_detections:\n",
    "        print(\"No detections to analyze\")\n",
    "        return\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(all_detections)\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Class distribution\n",
    "    class_counts = df['class_name'].value_counts()\n",
    "    ax1.pie(class_counts.values, labels=[name.replace('_', ' ').title() for name in class_counts.index], \n",
    "            autopct='%1.1f%%', startangle=90)\n",
    "    ax1.set_title('Detection Distribution by Class', fontweight='bold')\n",
    "    \n",
    "    # 2. Confidence distribution\n",
    "    ax2.hist(df['confidence'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax2.set_xlabel('Confidence Score')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Confidence Score Distribution', fontweight='bold')\n",
    "    ax2.axvline(df['confidence'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {df[\"confidence\"].mean():.2f}')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 3. Box plot by class\n",
    "    df_plot = df.copy()\n",
    "    df_plot['class_name'] = df_plot['class_name'].str.replace('_', ' ').str.title()\n",
    "    sns.boxplot(data=df_plot, x='class_name', y='confidence', ax=ax3)\n",
    "    ax3.set_title('Confidence by Class', fontweight='bold')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. Detection counts by class\n",
    "    class_counts.plot(kind='bar', ax=ax4, color='lightcoral')\n",
    "    ax4.set_title('Detection Counts by Class', fontweight='bold')\n",
    "    ax4.set_xlabel('Anomaly Type')\n",
    "    ax4.set_ylabel('Count')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nðŸ“Š Summary Statistics:\")\n",
    "    print(f\"Total detections: {len(df)}\")\n",
    "    print(f\"Average confidence: {df['confidence'].mean():.2f}\")\n",
    "    print(f\"Most common anomaly: {class_counts.index[0].replace('_', ' ').title()}\")\n",
    "    print(f\"Confidence range: {df['confidence'].min():.2f} - {df['confidence'].max():.2f}\")\n",
    "\n",
    "print(\"âœ… Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process Sample Images\n",
    "\n",
    "Let's process some sample images to demonstrate the detector's capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sample image paths (add your own images here)\n",
    "sample_images = [\n",
    "    \"../data/raw/sample1.jpg\",\n",
    "    \"../data/raw/sample2.jpg\", \n",
    "    \"../data/raw/sample3.jpg\"\n",
    "]\n",
    "\n",
    "# Check which images exist\n",
    "existing_images = [img for img in sample_images if Path(img).exists()]\n",
    "\n",
    "if not existing_images:\n",
    "    print(\"âš ï¸  No sample images found in ../data/raw/\")\n",
    "    print(\"Please add some images to the data/raw/ directory to test the detector.\")\n",
    "    print(\"\\nðŸ’¡ You can download sample civic images from:\")\n",
    "    print(\"  - Google Images (search for 'potholes', 'garbage dump', etc.)\")\n",
    "    print(\"  - Unsplash.com\")\n",
    "    print(\"  - Your own photos\")\n",
    "else:\n",
    "    print(f\"âœ… Found {len(existing_images)} sample images to process\")\n",
    "    for img in existing_images:\n",
    "        print(f\"  - {Path(img).name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each sample image\n",
    "all_detections = []\n",
    "confidence_threshold = 0.5\n",
    "\n",
    "print(f\"ðŸ” Processing images with confidence threshold: {confidence_threshold}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, image_path in enumerate(existing_images, 1):\n",
    "    print(f\"\\nðŸ“¸ Processing image {i}/{len(existing_images)}: {Path(image_path).name}\")\n",
    "    \n",
    "    # Run detection\n",
    "    result_image, detections = detector.detect_anomalies(image_path, confidence_threshold)\n",
    "    \n",
    "    if result_image is not None:\n",
    "        # Display results\n",
    "        display_detection_results(image_path, result_image, detections)\n",
    "        \n",
    "        # Collect all detections for statistics\n",
    "        all_detections.extend(detections)\n",
    "        \n",
    "        # Generate report\n",
    "        if detections:\n",
    "            report = create_detection_report(detections, image_path)\n",
    "            print(\"\\nðŸ“‹ Detection Report:\")\n",
    "            print(report)\n",
    "    else:\n",
    "        print(f\"âŒ Failed to process {Path(image_path).name}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Processing complete! Total detections across all images: {len(all_detections)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Overall Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comprehensive statistics\n",
    "if all_detections:\n",
    "    print(\"ðŸ“Š Analyzing detection statistics across all processed images...\")\n",
    "    plot_detection_statistics(all_detections)\n",
    "    \n",
    "    # Additional analysis\n",
    "    stats = calculate_detection_stats(all_detections)\n",
    "    \n",
    "    print(\"\\nðŸ” Detailed Analysis:\")\n",
    "    print(f\"Images processed: {len(existing_images)}\")\n",
    "    print(f\"Total anomalies detected: {stats['total_count']}\")\n",
    "    print(f\"Average anomalies per image: {stats['total_count'] / len(existing_images):.1f}\")\n",
    "    print(f\"Average confidence: {stats['avg_confidence']:.2f}\")\n",
    "    \n",
    "    print(\"\\nðŸ† Top Detected Anomalies:\")\n",
    "    for class_name, count in sorted(stats['class_counts'].items(), key=lambda x: x[1], reverse=True):\n",
    "        percentage = (count / stats['total_count']) * 100\n",
    "        print(f\"  {class_name.replace('_', ' ').title()}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "else:\n",
    "    print(\"No detections found to analyze. Try:\")\n",
    "    print(\"1. Adding more sample images to data/raw/\")\n",
    "    print(\"2. Lowering the confidence threshold\")\n",
    "    print(\"3. Using images with visible civic issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interactive Detection\n",
    "\n",
    "Try different confidence thresholds to see how they affect detection results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive confidence threshold testing\n",
    "if existing_images:\n",
    "    test_image = existing_images[0]  # Use first available image\n",
    "    confidence_levels = [0.3, 0.5, 0.7, 0.9]\n",
    "    \n",
    "    print(f\"ðŸ§ª Testing different confidence thresholds on: {Path(test_image).name}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    results_by_confidence = {}\n",
    "    \n",
    "    for conf in confidence_levels:\n",
    "        result_image, detections = detector.detect_anomalies(test_image, conf)\n",
    "        results_by_confidence[conf] = len(detections)\n",
    "        \n",
    "        print(f\"\\nConfidence {conf}: {len(detections)} detections\")\n",
    "        if detections:\n",
    "            for det in detections:\n",
    "                print(f\"  - {det['class_name'].replace('_', ' ').title()}: {det['confidence']:.2f}\")\n",
    "    \n",
    "    # Plot confidence threshold analysis\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(confidence_levels, list(results_by_confidence.values()), \n",
    "             marker='o', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Confidence Threshold')\n",
    "    plt.ylabel('Number of Detections')\n",
    "    plt.title('Detection Count vs Confidence Threshold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Observations:\")\n",
    "    print(\"- Lower thresholds detect more objects but may include false positives\")\n",
    "    print(\"- Higher thresholds are more conservative but may miss some detections\")\n",
    "    print(\"- Choose threshold based on your use case (precision vs recall)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model performance if validation data is available\n",
    "val_images_dir = Path(\"../data/processed/val/images\")\n",
    "\n",
    "if val_images_dir.exists() and any(val_images_dir.iterdir()):\n",
    "    print(\"ðŸ“Š Running model validation on validation set...\")\n",
    "    \n",
    "    # Get validation images\n",
    "    val_images = list(val_images_dir.glob(\"*.jpg\")) + list(val_images_dir.glob(\"*.png\"))\n",
    "    \n",
    "    if val_images:\n",
    "        print(f\"Found {len(val_images)} validation images\")\n",
    "        \n",
    "        # Process a subset for demo (first 5 images)\n",
    "        sample_val_images = val_images[:5]\n",
    "        \n",
    "        val_detections = []\n",
    "        processing_times = []\n",
    "        \n",
    "        for img_path in sample_val_images:\n",
    "            import time\n",
    "            start_time = time.time()\n",
    "            \n",
    "            result_image, detections = detector.detect_anomalies(str(img_path), 0.5)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            processing_time = end_time - start_time\n",
    "            processing_times.append(processing_time)\n",
    "            \n",
    "            if detections:\n",
    "                val_detections.extend(detections)\n",
    "        \n",
    "        # Performance metrics\n",
    "        avg_processing_time = np.mean(processing_times)\n",
    "        fps = 1 / avg_processing_time\n",
    "        \n",
    "        print(f\"\\nâš¡ Performance Metrics:\")\n",
    "        print(f\"Average processing time: {avg_processing_time:.3f} seconds\")\n",
    "        print(f\"Frames per second (FPS): {fps:.1f}\")\n",
    "        print(f\"Total detections in validation sample: {len(val_detections)}\")\n",
    "        print(f\"Average detections per image: {len(val_detections) / len(sample_val_images):.1f}\")\n",
    "        \n",
    "        # Plot processing times\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.bar(range(len(processing_times)), processing_times, alpha=0.7)\n",
    "        plt.axhline(avg_processing_time, color='red', linestyle='--', \n",
    "                   label=f'Average: {avg_processing_time:.3f}s')\n",
    "        plt.xlabel('Image Index')\n",
    "        plt.ylabel('Processing Time (seconds)')\n",
    "        plt.title('Processing Time per Image')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "else:\n",
    "    print(\"âš ï¸  No validation images found. To run performance analysis:\")\n",
    "    print(\"1. Add images to data/processed/val/images/\")\n",
    "    print(\"2. Or run the data preparation script first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to files\n",
    "if all_detections:\n",
    "    # Create results directory\n",
    "    results_dir = Path(\"../results\")\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Export to CSV\n",
    "    df_results = pd.DataFrame(all_detections)\n",
    "    csv_path = results_dir / \"detection_results.csv\"\n",
    "    df_results.to_csv(csv_path, index=False)\n",
    "    print(f\"âœ… Results exported to: {csv_path}\")\n",
    "    \n",
    "    # Export summary statistics\n",
    "    stats = calculate_detection_stats(all_detections)\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "CIVIC ANOMALY DETECTION SUMMARY\n",
    "==============================\n",
    "\n",
    "Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Images Processed: {len(existing_images)}\n",
    "Total Detections: {stats['total_count']}\n",
    "Average Confidence: {stats['avg_confidence']:.2f}\n",
    "\n",
    "DETECTION BREAKDOWN:\n",
    "\"\"\"\n",
    "    \n",
    "    for class_name, count in stats['class_counts'].items():\n",
    "        percentage = (count / stats['total_count']) * 100\n",
    "        summary_text += f\"- {class_name.replace('_', ' ').title()}: {count} ({percentage:.1f}%)\\n\"\n",
    "    \n",
    "    summary_text += f\"\"\"\n",
    "CONFIDENCE DISTRIBUTION:\n",
    "- High confidence (>0.8): {stats['confidence_distribution']['high (>0.8)']}\n",
    "- Medium confidence (0.5-0.8): {stats['confidence_distribution']['medium (0.5-0.8)']}\n",
    "- Low confidence (<0.5): {stats['confidence_distribution']['low (<0.5)']}\n",
    "\"\"\"\n",
    "    \n",
    "    summary_path = results_dir / \"detection_summary.txt\"\n",
    "    with open(summary_path, 'w') as f:\n",
    "        f.write(summary_text)\n",
    "    \n",
    "    print(f\"âœ… Summary exported to: {summary_path}\")\n",
    "    print(\"\\nðŸ“ Results saved in ../results/ directory\")\n",
    "    \n",
    "else:\n",
    "    print(\"No results to export. Process some images first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Conclusion\n",
    "\n",
    "This notebook demonstrated the key capabilities of the Civic Image Anomaly Detector:\n",
    "\n",
    "### âœ… What We Accomplished\n",
    "1. **Model Loading**: Successfully loaded and initialized the detector\n",
    "2. **Image Processing**: Processed sample images and visualized results\n",
    "3. **Statistical Analysis**: Analyzed detection patterns and confidence scores\n",
    "4. **Performance Testing**: Measured processing speed and accuracy\n",
    "5. **Results Export**: Saved results for further analysis\n",
    "\n",
    "### ðŸš€ Next Steps\n",
    "1. **Add More Images**: Test with diverse civic images\n",
    "2. **Fine-tune Model**: Train on custom dataset for better accuracy\n",
    "3. **Deploy Application**: Use Streamlit app or API for production\n",
    "4. **Integration**: Connect with civic reporting systems\n",
    "\n",
    "### ðŸ’¡ Tips for Better Results\n",
    "- Use high-quality, well-lit images\n",
    "- Ensure anomalies are clearly visible\n",
    "- Adjust confidence threshold based on use case\n",
    "- Train on domain-specific data for best performance\n",
    "\n",
    "---\n",
    "\n",
    "**Happy detecting! ðŸ™ï¸âœ¨**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}